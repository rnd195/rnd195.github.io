[
  {
    "objectID": "privacy.html",
    "href": "privacy.html",
    "title": "Privacy policy",
    "section": "",
    "text": "I (Martin Řanda or rnd195) do not collect any personal data from visitors of this website.\nThis website is hosted using GitHub Pages. Please note that the host may collect personal data as described in the documentation for GitHub Pages:\n\nWhen a GitHub Pages site is visited, the visitor’s IP address is logged and stored for security purposes, regardless of whether the visitor has signed into GitHub or not. For more information about GitHub’s security practices, see GitHub Privacy Statement.\n\nThis website may contain links to third-party websites. Please note that once you leave this site and access these external websites, this privacy policy no longer applies.\nI may update this privacy policy from time to time. Any changes will be posted on this page with a revised effective date. If you have any questions or concerns regarding this privacy policy, please contact me at contact@rnd195.anonaddy.com.\nLast updated: 1 Jan 2025"
  },
  {
    "objectID": "posts/reflections-fosdem-2025/index.html",
    "href": "posts/reflections-fosdem-2025/index.html",
    "title": "Reflections on FOSDEM 2025",
    "section": "",
    "text": "Briefly reflecting on my first-ever attendance of the FOSDEM conference"
  },
  {
    "objectID": "posts/reflections-fosdem-2025/index.html#about-the-event-and-my-takeaways",
    "href": "posts/reflections-fosdem-2025/index.html#about-the-event-and-my-takeaways",
    "title": "Reflections on FOSDEM 2025",
    "section": "About the event and my takeaways",
    "text": "About the event and my takeaways\nFOSDEM1 is the largest conference about free and open source software.2 This year’s event was held for the 25th time in the first two days of February, and I was fortunate enough to be attending. It was my first time participating in the event, and what I was initially struck by (in a positive way) was the conference’s turnout. In fact, so many people showed up to the event that, on several occasions throughout the first day, my connection to the cellular network had trouble keeping up.\nAmong other things, the event consisted of talks/lectures/panels on topics such as Android, Python, Security, and 70+ other tracks.3 I overheard some people describing the event as chaotic and while I get that the sheer size of the conference could give this impression, I feel like calling it “chaotic” takes away from the impressive level of planning and organization demonstrated by the team behind the conference. For instance, each talk was streamed live and recorded, every block that I attended was on schedule, the event even had companion apps, and so much more.\nThe topics that I was mainly interested in were centered around data science, Python, as well as legal & policy. I particularly enjoyed the panel discussion on what it means for an AI system or a machine learning model to be free/open.4 It was also great to meet some of the people behind the PyArrow5 package—I typically use PyArrow for data storage via the .parquet file format, and while this is only a small portion of the package’s capabilities, I consider it to be such a time (& disk space) saver. Last but not least, I attended a talk about commercially viable companies that focus on developing open source software and what it takes to be respected by the open source community.6"
  },
  {
    "objectID": "posts/reflections-fosdem-2025/index.html#a-first-timers-tips-for-future-first-timers",
    "href": "posts/reflections-fosdem-2025/index.html#a-first-timers-tips-for-future-first-timers",
    "title": "Reflections on FOSDEM 2025",
    "section": "A first-timer’s tips for future first-timers",
    "text": "A first-timer’s tips for future first-timers\nBefore the introductory keynote speech began, I asked an experienced FOSDEM-goer who sat next to me for tips on how to get the most out of the conference. The gentleman said that there is no wrong way to experience the event, and it’s really up to each attendee to make the most out of it.\nFrom my experience, it’s best to be prepared for the highly likely possibility that you won’t be able to visit all the talks that you planned to visit due to time and room capacity constraints. Therefore, I would suggest choosing a few developer rooms that you can retreat to if things don’t go according to the plan. Each talk is live-streamed and recorded (available a week or so after the event), so I wouldn’t really worry about missing a particular talk. I think that the added value of this event is that it is in-person. I would instead focus on that.\nFor more tips and insights about the event in general, allow me to recommend this detailed write-up on the ./techtipsy blog about last year’s event.\n Comment on GitHub"
  },
  {
    "objectID": "posts/reflections-fosdem-2025/index.html#footnotes",
    "href": "posts/reflections-fosdem-2025/index.html#footnotes",
    "title": "Reflections on FOSDEM 2025",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://fosdem.org/↩︎\nhttps://fosdem.org/2025/schedule/event/fosdem-2025-6712-welcome-to-fosdem-2025/, 5:40↩︎\nhttps://fosdem.org/2025/schedule/#devrooms↩︎\nhttps://fosdem.org/2025/schedule/event/fosdem-2025-6639-panel-when-is-an-ai-system-free-open-/↩︎\nhttps://fosdem.org/2025/schedule/event/fosdem-2025-6092-what-can-pyarrow-do-for-you-array-interchange-storage-compute-and-transport/↩︎\nhttps://fosdem.org/2025/schedule/event/fosdem-2025-5320-build-a-great-business-on-open-source-without-selling-your-soul/↩︎"
  },
  {
    "objectID": "posts/build-package-uv/index.html",
    "href": "posts/build-package-uv/index.html",
    "title": "How to Build a Local Python Package with uv",
    "section": "",
    "text": "A walkthrough of how to build your custom Python package using uv without publishing it.\nBlink and you’ll miss it. Astral’s uv is a performant package & project manager that’s out to replace all the competing tools.\nIn this blog post, I’ll walk you through a minimal example of how to build your own Python package without uploading it to the internet. While it’s great to get feedback from others when sharing your code, there may be simple reasons why that’s not always possible. For instance, if you’re building an internal package at work or testing something out, it’s usually not for sharing."
  },
  {
    "objectID": "posts/build-package-uv/index.html#get-started",
    "href": "posts/build-package-uv/index.html#get-started",
    "title": "How to Build a Local Python Package with uv",
    "section": "Get started",
    "text": "Get started\nFirst, let’s start by installing uv on your machine. Assuming you have Python version 3.8 or later installed, an easy way to install uv is to use pip. First, create a folder for your project (e.g., my-internal-pkg), open up a terminal inside, and create a virtual environment:\npython -m venv .venv\nActivate the environment and install uv using pip. I’m using PowerShell, so I run the following:\n.venv\\Scripts\\activate\npip install uv\nOnce uv is installed (v0.8.4 used), initialize your project by running:\nuv init\nNow, the directory of your project should look like this:\nmy-internal-pkg/\n├── .venv/\n├── .gitignore\n├── .python-version\n├── main.py\n├── pyproject.toml\n└── README.md\nAs we can see, uv init created 5 files and initialized a git repository in the project folder. The important file to focus on is pyproject.toml. This file configures your future package based on what you specify inside. Let’s take a peek at the default structure of this file:\n[project]\nname = \"my-internal-pkg\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.12\"\ndependencies = []\nMost of the settings in your pyproject.toml are generated and managed by uv. Of course, all settings in this text file can be modified manually. I would, however, advise against making manual changes except for options like description or when adding whole sections (we’ll touch on that later). Let’s leave these settings unchanged for now."
  },
  {
    "objectID": "posts/build-package-uv/index.html#package-contents",
    "href": "posts/build-package-uv/index.html#package-contents",
    "title": "How to Build a Local Python Package with uv",
    "section": "Package contents",
    "text": "Package contents\nAs a minimal example, let me create a simple Python script that calculates rolling minimum and maximum values for a given column in a polars dataframe. If you’re unfamiliar with polars, consider reading my guide on getting started with polars as a pandas user.\nFirst, let me install polars and set it as a dependency for my package:\nuv add polars\nThis command installs the package in the virtual environment created earlier and modifies pyproject.toml by adding polars as a dependency. If we take a look at the pyproject.toml file now, the dependencies setting should look something like this\ndependencies = [\n    \"polars&gt;=1.31.0\",\n]\nNext, let me delete the main.py file since it’s not needed. I’ll then create a folder inside the root directory of my project and name it my_internal_pkg. I’ll also place an empty __init__.py file inside the newly created folder.\nMy goal is to write a function that calculates rolling min and max values—I’m going to place this function in df_helpers.py. Here’s the updated file tree:\nmy-internal-pkg/\n├── .venv/\n├── my_internal_pkg/\n│   ├── __init__.py\n│   └── df_helpers.py\n├── .gitignore\n├── .python-version\n├── pyproject.toml\n└── README.md\nNow onto the contents of df_helpers.py. Keeping it really simple, below is the rolling min max function I placed in this Python script:\nimport polars as pl\n\n\ndef rolling_min_max(df: pl.DataFrame, col: str, window: int = 3) -&gt; pl.DataFrame:\n    \"\"\"Get rolling min and max values of a specific column\"\"\"\n    return df.with_columns(\n        pl.col(col).rolling_min(window_size=window).alias(f\"{col}_roll_min\"),\n        pl.col(col).rolling_max(window_size=window).alias(f\"{col}_roll_max\")\n    )\nNow that we have something to be packaged, we’re almost set. There are some final touches we need to make to the pyproject.toml file."
  },
  {
    "objectID": "posts/build-package-uv/index.html#finalizing-pyprojects.toml",
    "href": "posts/build-package-uv/index.html#finalizing-pyprojects.toml",
    "title": "How to Build a Local Python Package with uv",
    "section": "Finalizing pyprojects.toml",
    "text": "Finalizing pyprojects.toml\nSince this tutorial is aimed at building a package locally, consider adding classifiers = [\"Private :: Do Not Upload\"] into the [project] section in the config file:\n[project]\nname = \"my-internal-pkg\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.12\"\ndependencies = [\n    \"polars&gt;=1.31.0\",\n]\nclassifiers = [\"Private :: Do Not Upload\"]\nThis labels the package as “private” and adds another layer of defense against accidentally publishing your package to PyPI (see docs).\nNext, let’s set up the [build-system] section. uv’s own backend has recently been marked as stable, and it seems reasonable to believe that it’s going to become the default in the future. In the meantime, let me use hatchling from the Python Packaging Authority, which uv authors considered reasonable as a build backed. As a sidenote, I was curious which prominent projects also use this build system: JupyterLab, for example.\nTo use hatchling as the build backend, we need to place the following section into the .toml file\n[build-system]\nrequires = [\"hatchling &gt;= 1.26\"]\nbuild-backend = \"hatchling.build\"\nAdditionally, we have to specify the folder containing the core of our package (my_internal_pkg/ in our case) because the build system expects to find the src/ folder by default. Many prominent projects like django or pandas don’t follow these defaults either and instead place their scripts into django and pandas subfolders, respectively.\nConsequently, we also need to add 1 more section to pyproject.toml:\n[tool.hatch.build.targets.wheel]\npackages = [\"my_internal_pkg/\"]\nFor additional configuration like excluding files/folders in the source distribution, consider taking a look at hatch docs.\nWe’re almost at the finish line. For completeness, the final pyproject.toml file looks like this:\n[project]\nname = \"my-internal-pkg\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.12\"\ndependencies = [\n    \"polars&gt;=1.31.0\",\n]\nclassifiers = [\"Private :: Do Not Upload\"]\n\n[build-system]\nrequires = [\"hatchling &gt;= 1.26\"]\nbuild-backend = \"hatchling.build\"\n\n[tool.hatch.build.targets.wheel]\npackages = [\"my_internal_pkg/\"]"
  },
  {
    "objectID": "posts/build-package-uv/index.html#build",
    "href": "posts/build-package-uv/index.html#build",
    "title": "How to Build a Local Python Package with uv",
    "section": "Build",
    "text": "Build\nWith pyproject.toml set up, we should now be able to build the package. What’s going to happen now is that the build command will create a folder named dist/. Inside this folder, we’re going to see our package in a .tar.gz (source distribution) and .whl (wheel) format. While the difference between these is beyond the scope of this post, if you’re going to be distributing your package locally—to other colleagues or your other systems—consider using the .whl file because it simplifies the process of installing the package. Note that both of these are ultimately archives, so you can take a look inside each to see the differences. To learn more about this topic, I would recommend reading this excellent article on Real Python.\nEnough stalling, let’s build the package. Open up a terminal in the root directory of the project, and simply type:\nuv build\nIf successful, the dist/ folder should pop up and the two distributions should be inside:\ndist/\n├── .gitignore\n├── my_internal_pkg-0.1.0.tar.gz\n└── my_internal_pkg-0.1.0-py3-none-any.whl"
  },
  {
    "objectID": "posts/build-package-uv/index.html#install",
    "href": "posts/build-package-uv/index.html#install",
    "title": "How to Build a Local Python Package with uv",
    "section": "Install",
    "text": "Install\nTo install the package, grab the .whl file and place it wherever you need it installed. You can then install it using pip (or uv pip) by running:\npip install \"path/to/my_internal_pkg-0.1.0-py3-none-any.whl\""
  },
  {
    "objectID": "posts/build-package-uv/index.html#thats-it",
    "href": "posts/build-package-uv/index.html#thats-it",
    "title": "How to Build a Local Python Package with uv",
    "section": "That’s it?",
    "text": "That’s it?\nYes, that’s it! With just a few commands and a handful of lines of configuration, we’ve built a Python package from scratch using uv.\nYou should now be able to import your package in the environment you installed it to using\nimport my_internal_pkg\nThis was just a minimal example of how to get started building packages. Consider browsing through repositories of popular Python packages to see how they do it. Additionally, for a tutorial on how to publish your package, take a look at uv docs.\nIf you have any suggestions or corrections, feel free to let me know in the comments on GitHub!\n Comment on GitHub"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Martin Řanda",
    "section": "",
    "text": "Hi, I’m Martin Řanda.\nI work on forecasting models, data processing, and interactive visualizations in a major Czech energy trading company based in Prague. I have a Master’s degree in Financial Markets and Data Analysis from Charles University.\nThis is what I have been up to recently:\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nAug 3, 2025\n\n\nHow to Build a Local Python Package with uv\n\n\n\n\nMay 5, 2025\n\n\nA Beginner’s Guide to pandas & polars\n\n\n\n\nFeb 15, 2025\n\n\nReflections on FOSDEM 2025\n\n\n\n\nJan 26, 2025\n\n\nBenchmarking TimesFM on Electricity Consumption Data\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "rnd195’s blog",
    "section": "",
    "text": "How to Build a Local Python Package with uv\n\n\n\npython\n\n\n\n\n\n\n\n\n\nAug 3, 2025\n\n\nMartin Řanda\n\n\n\n\n\n\n\n\n\n\n\n\nA Beginner’s Guide to pandas & polars\n\n\n\npython\n\n\n\n\n\n\n\n\n\nMay 5, 2025\n\n\nMartin Řanda\n\n\n\n\n\n\n\n\n\n\n\n\nReflections on FOSDEM 2025\n\n\n\nevents\n\n\n\n\n\n\n\n\n\nFeb 15, 2025\n\n\nMartin Řanda\n\n\n\n\n\n\n\n\n\n\n\n\nBenchmarking TimesFM on Electricity Consumption Data\n\n\n\nanalysis\n\npython\n\n\n\n\n\n\n\n\n\nJan 26, 2025\n\n\nMartin Řanda\n\n\n\n\n\nNo matching items\n\nOpinions expressed on my blog are my own."
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "Text, images, code, or other resources that I (Martin Řanda or rnd195) produced and posted on my blog are licensed under CC BY 4.0 unless stated otherwise.\nThe code that I produced to create this website is licensed under the MIT license.\nThe Inter font used on this website is made by the Inter Project Authors and is licensed under the SIL Open Font License 1.1.\nThe tool used to generate this website, Quarto, is licensed under the MIT license.\n\nAll relevant license files can be also found within the GitHub repository of this website in the licenses folder."
  },
  {
    "objectID": "posts/my-pd-pl-guide/index.html",
    "href": "posts/my-pd-pl-guide/index.html",
    "title": "A Beginner’s Guide to pandas & polars",
    "section": "",
    "text": "This guide will help you start learning polars by showcasing analogous code snippets from pandas.\nIn recent years, polars1 is becoming increasingly popular in the data science community (more than 33k stars on GitHub as of May 20252). According to the author of polars, Ritchie Vink, the package’s API is “consistent and strict,” and its focus is on maximizing single machine performance3 which perhaps explains some of the library’s appeal. From my experience, polars has been a major time saver, especially in data-intensive computations. However, I think that it is perfectly reasonable to prefer pandas for some tasks (like quick data visualization), and I am glad that this competition is pushing the field forward.\nIn this post, I wrote down some of the most common operations in pandas and their equivalents in polars to help you get acquainted with the package (and to help myself remember). Please, note that this guide / cheat sheet may not be exhaustive and in some cases, there might be additional ways to achieve the same goal. Feel free to let me know in the comments.\nThis is a runnable Quarto document, so first, let’s load the packages.\nimport pandas as pd\nimport numpy as np\nimport polars as pl\nfrom datetime import date, timedelta, datetime"
  },
  {
    "objectID": "posts/my-pd-pl-guide/index.html#load-data",
    "href": "posts/my-pd-pl-guide/index.html#load-data",
    "title": "A Beginner’s Guide to pandas & polars",
    "section": "Load data",
    "text": "Load data\nWe’ll be working with my Wizard Shop Dataset4 which was specifically crafted for introductory data analysis. It consists of three tables:\n\nwizard_shop_inventory.csv: A list of products with prices, item quality, and other attributes.\nmagical_items_info.csv: A small table with typical price, quality, and where the item is typically found.\nitems_prices_timeline.csv: Average daily prices of each product category.\n\nLet’s load the data.\n\n1️⃣ pandas2️⃣ polars\n\n\nAs we can see, the syntax is the same in both packages except for parsing dates.\n\ndata_url = \"https://raw.githubusercontent.com/rnd195/wizard-shop-dataset/refs/heads/main/data/\"\n\ndf_pd = pd.read_csv(data_url + \"wizard_shop_inventory.csv\")\ninfo_pd = pd.read_csv(data_url + \"magical_items_info.csv\")\nprices_pd = pd.read_csv(\n    data_url + \"items_prices_timeline.csv\", \n    parse_dates=[\"date\"]\n)\n\n\n\nAs we can see, the syntax is the same in both packages except for parsing dates.\n\ndata_url = \"https://raw.githubusercontent.com/rnd195/wizard-shop-dataset/refs/heads/main/data/\"\n\ndf_pl = pl.read_csv(data_url + \"wizard_shop_inventory.csv\")\ninfo_pl = pl.read_csv(data_url + \"magical_items_info.csv\")\nprices_pl = pl.read_csv(\n    data_url + \"items_prices_timeline.csv\", \n    try_parse_dates=True\n)"
  },
  {
    "objectID": "posts/my-pd-pl-guide/index.html#take-a-peek",
    "href": "posts/my-pd-pl-guide/index.html#take-a-peek",
    "title": "A Beginner’s Guide to pandas & polars",
    "section": "Take a peek",
    "text": "Take a peek\nSometimes, we want to take a quick look at the data. The methods .sample(), .head(), and .tail() all work in both packages.\n\n1️⃣ pandas2️⃣ polars\n\n\nThe df DataFrame contains all the products the wizard shopkeeper sells—items like potions, amulets, or cloaks.\n\ndf_pd.sample(3)\n\n\n\n\n\n\n\n\nid\nitem\nprice\nmagical_power\nquality\nin_stock\nfound_in\n\n\n\n\n426\n427\ncloak\n630.5\n298.183\n0\nTrue\ndungeon\n\n\n299\n300\nstaff\n2124.0\n772.533\n7\nTrue\nNaN\n\n\n42\n43\namulet\n1076.0\n465.399\n9\nTrue\ndungeon\n\n\n\n\n\n\n\nThe info table contains information about the typical attributes of these items.\n\ninfo_pd.head(3)\n\n\n\n\n\n\n\n\nitem\ntypical_price\ntypical_quality\ntypically_found_in\n\n\n\n\n0\namulet\n1000\n9\ndungeon\n\n\n1\npotion\n50\n7\nvillage\n\n\n2\ncloak\n500\n4\ncity\n\n\n\n\n\n\n\nThe prices DataFrame contains the daily average price of each item in the fantasy world’s economy in the magical year of 2025.\n\nprices_pd.tail(3)\n\n\n\n\n\n\n\n\ndate\namulet\npotion\ncloak\nstaff\nscroll\n\n\n\n\n362\n2025-12-29\n742.21\n44.70\n648.72\n971.90\n731.69\n\n\n363\n2025-12-30\n802.06\n48.99\n446.10\n1711.04\n728.60\n\n\n364\n2025-12-31\n957.94\n64.08\n503.88\n2899.72\n829.59\n\n\n\n\n\n\n\n\n\nThe df DataFrame contains all the products the wizard shopkeeper sells—items like potions, amulets, or cloaks.\n\ndf_pl.sample(3)\n\n\nshape: (3, 7)\n\n\n\nid\nitem\nprice\nmagical_power\nquality\nin_stock\nfound_in\n\n\ni64\nstr\nf64\nf64\ni64\nbool\nstr\n\n\n\n\n376\n\"scroll\"\n1039.5\n424.913\n4\ntrue\n\"dungeon\"\n\n\n90\n\"potion\"\n64.35\n93.7195\n3\ntrue\n\"dungeon\"\n\n\n125\n\"potion\"\n54.05\n108.9175\n8\ntrue\n\"dungeon\"\n\n\n\n\n\n\nThe info table contains information about the typical attributes of these items.\n\ninfo_pl.head(3)\n\n\nshape: (3, 4)\n\n\n\nitem\ntypical_price\ntypical_quality\ntypically_found_in\n\n\nstr\ni64\ni64\nstr\n\n\n\n\n\"amulet\"\n1000\n9\n\"dungeon\"\n\n\n\"potion\"\n50\n7\n\"village\"\n\n\n\"cloak\"\n500\n4\n\"city\"\n\n\n\n\n\n\nThe prices DataFrame contains the daily average price of each item in the fantasy world’s economy in the magical year of 2025.\n\nprices_pl.tail(3)\n\n\nshape: (3, 6)\n\n\n\ndate\namulet\npotion\ncloak\nstaff\nscroll\n\n\ndate\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n2025-12-29\n742.21\n44.7\n648.72\n971.9\n731.69\n\n\n2025-12-30\n802.06\n48.99\n446.1\n1711.04\n728.6\n\n\n2025-12-31\n957.94\n64.08\n503.88\n2899.72\n829.59"
  },
  {
    "objectID": "posts/my-pd-pl-guide/index.html#subset-a-dataframe",
    "href": "posts/my-pd-pl-guide/index.html#subset-a-dataframe",
    "title": "A Beginner’s Guide to pandas & polars",
    "section": "Subset a DataFrame",
    "text": "Subset a DataFrame\n\nColumns\n\nSelect a column by name\nThere are several ways to select a single column in both pandas and polars. Note that some of the calls return a Series while others return a DataFrame.\n\n1️⃣ pandas2️⃣ polars\n\n\n\ndf_pd[\"price\"]        # -&gt; returns Series of shape (500,)\ndf_pd[[\"price\"]]      # -&gt; returns DataFrame of shape (500, 1)\ndf_pd.price           # -&gt; returns Series of shape (500,)\ndf_pd.loc[:, \"price\"] # -&gt; returns Series of shape (500,)\n\n\n\n\ndf_pl[\"price\"]                # -&gt; returns Series of shape (500,)\ndf_pl[:, \"price\"]             # -&gt; returns Series of shape (500,)\ndf_pl.select(\"price\")         # -&gt; returns DataFrame of shape (500, 1)\ndf_pl.select(pl.col(\"price\")) # -&gt; returns DataFrame of shape (500, 1)\n\n\n\n\n\n\nSelect multiple columns by name\nBelow are several alternatives for selecting columns.\n\n1️⃣ pandas2️⃣ polars\n\n\n\ndf_pd[[\"item\", \"price\"]]\ndf_pd.loc[:, [\"item\", \"price\"]]\n\n\n\n\ndf_pl[[\"item\", \"price\"]]\ndf_pl[:, [\"item\", \"price\"]]\ndf_pl[\"item\", \"price\"]\ndf_pl.select([\"item\", \"price\"])\ndf_pl.select(pl.col(\"item\", \"price\"))\n\n\n\n\n\n\nSlice columns by range\nInstead of selecting columns by name, we can write their positions in the DataFrame.\n\n1️⃣ pandas2️⃣ polars\n\n\n\ndf_pd.iloc[:, 5:7]\n\n\n\n\ndf_pl[:, 5:7]\n\n\n\n\n\n\nSlice columns by name\nIt’s also possible to select a range of columns by name. The resulting DataFrame will contain the first and the last selected column as well as any columns in-between.\n\n1️⃣ pandas2️⃣ polars\n\n\n\ndf_pd.loc[:, \"in_stock\":\"found_in\"]\n\n\n\n\ndf_pl[:, \"in_stock\":\"found_in\"]\n\n\n\n\n\n\nFilter columns using Bools\nWe can pass a list of True/False values to select specific columns. The length of this list needs to be the same as the number of columns in the DataFrame. For instance, df_pd/df_pl contains 7 columns. Thus, one possible list of True/False values may look like this: [True, False, True, False, True, True, True].\n\n1️⃣ pandas2️⃣ polars\n\n\n\n# Return all columns containing the substring \"price\"\ndf_pd.loc[:, [\"price\" in col for col in df_pd.columns]]\n\n\n\n\n# Return all columns containing the substring \"price\"\ndf_pl[:, [\"price\" in col for col in df_pl.columns]]\n\n\n\n\n\n\n\nRows\n\nSelect row by index label\nIn case the index is, for example, datetime or str, it is possible to select rows by the index label. However, this is not applicable in polars since polars treats the index differently than pandas.\n\n1️⃣ pandas2️⃣ polars\n\n\n\nprices_pd = prices_pd.set_index(\"date\")\nprices_pd.loc[\"2025-01-05\"]\n\namulet     996.24\npotion      49.65\ncloak      497.42\nstaff     2643.03\nscroll    1096.03\nName: 2025-01-05 00:00:00, dtype: float64\n\n\n\n\n\n# Not applicable in polars\n# Below is a call that outputs a similar result\nprices_pl.filter(pl.col(\"date\") == date(2025, 1, 5))\n\n\nshape: (1, 6)\n\n\n\ndate\namulet\npotion\ncloak\nstaff\nscroll\n\n\ndate\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n2025-01-05\n996.24\n49.65\n497.42\n2643.03\n1096.03\n\n\n\n\n\n\n\n\n\n\n\nSelect a single row by integer position\nBoth pandas and polars support selecting a single row using its integer position.\n\n1️⃣ pandas2️⃣ polars\n\n\n\ndf_pd.iloc[4] # -&gt; returns a Series\ndf_pd[4:5]    # -&gt; returns a DataFrame\n\n\n\n\ndf_pl[4] \ndf_pl[4:5]\ndf_pl[4, :] # -&gt; all three of these return a DataFrame\n\n\n\n\n\n\nSlice rows by integer range\nLikewise, both pandas and polars support selecting rows using a range of integers.\n\n1️⃣ pandas2️⃣ polars\n\n\n\ndf_pd.iloc[0:5]\ndf_pd[0:5]\n\n\n\n\ndf_pl[0:5]\ndf_pl[0:5, :]\n\n\n\n\n\n\nFilter rows using Bools\nWe can pass a Series (or a similar object) containing True/False values to subset the DataFrame.\n\n1️⃣ pandas2️⃣ polars\n\n\n\n# Get products with price over 1000\ndf_pd[df_pd[\"price\"] &gt; 1000]\n\n\n\n\n# Get products with price over 1000\ndf_pl.filter(df_pl[\"price\"] &gt; 1000)\ndf_pl.filter(pl.col(\"price\") &gt; 1000)"
  },
  {
    "objectID": "posts/my-pd-pl-guide/index.html#creating-new-columns",
    "href": "posts/my-pd-pl-guide/index.html#creating-new-columns",
    "title": "A Beginner’s Guide to pandas & polars",
    "section": "Creating new columns",
    "text": "Creating new columns\n\nNew empty column\nSometimes, it might make sense to create a new column in a DataFrame and fill it with NA values. I think of it as “reserving” the column for values that will be put into the column later. There are several ways to achieve this in both packages.\n\n1️⃣ pandas2️⃣ polars\n\n\nMissing values in pandas: depends on the datatype. Consider using None or np.nan. Note that pd.NA is still experimental.\n\n# Is any item in the wizard's shop cursed? We don't know =&gt; NA\ndf_pd[\"is_cursed\"] = np.nan\ndf_pd = df_pd.assign(is_cursed=np.nan)\ndf_pd = df_pd.assign(**{\"is_cursed\": np.nan})\ndf_pd.loc[:, \"is_cursed\"] = np.nan\n\n\n\nMissing values in polars: None, represented as null.\n\n# Is any item in the wizard's shop cursed? We don't know =&gt; NA\ndf_pl = df_pl.with_columns(is_cursed=None)\ndf_pl = df_pl.with_columns(is_cursed=pl.lit(None))\ndf_pl = df_pl.with_columns(pl.lit(None).alias(\"is_cursed\"))\n\n\n\n\n\n\nTransform an existing column\nApply any function to an existing column in a DataFrame and write it as a new column.\n\n1️⃣ pandas2️⃣ polars\n\n\nSome options for transforming columns in pandas: .transform(), .apply(), calling a numpy function on the Series…\n\n# Take the logarithm of the price column\ndf_pd[\"log_price\"] = df_pd[\"price\"].transform(\"log\")\ndf_pd[\"log_price\"] = df_pd[\"price\"].apply(np.log)\ndf_pd[\"log_price\"] = np.log(df_pd[\"price\"])\n\n\n\nTransforming columns in polars: use the .with_columns(...) method.\n\n# Take the logarithm of the price column\ndf_pl = df_pl.with_columns(pl.col(\"price\").log().alias(\"log_price\"))\ndf_pl = df_pl.with_columns(log_price=pl.col(\"price\").log())\ndf_pl = df_pl.with_columns(df_pl[\"price\"].log().alias(\"log_price\"))"
  },
  {
    "objectID": "posts/my-pd-pl-guide/index.html#boolean-filters",
    "href": "posts/my-pd-pl-guide/index.html#boolean-filters",
    "title": "A Beginner’s Guide to pandas & polars",
    "section": "Boolean filters",
    "text": "Boolean filters\nUse set operations to filter the DataFrame based on predefined conditions.\n\n1️⃣ pandas2️⃣ polars\n\n\n\n# AND operator\ndf_pd[(df_pd[\"price\"] &gt; 500) & (df_pd[\"price\"] &lt; 2000)]\n# OR operator\ndf_pd[(df_pd[\"price\"] &gt;= 500) | (df_pd[\"item\"] != \"scroll\")]\n# Inverse\ndf_pd[~df_pd[\"in_stock\"]]\n# True if a value matches any of the specified values, else False\ndf_pd[df_pd[\"item\"].isin([\"staff\", \"potion\"])]\n# Is not NA... there's also .dropna() \ndf_pd[~df_pd[\"found_in\"].isna()]\n\n\n\n\n# AND operator\ndf_pl.filter((pl.col(\"price\") &gt; 500) & (pl.col(\"price\") &lt; 2000))\n# OR operator\ndf_pl.filter((pl.col(\"price\") &gt;= 500) | (pl.col(\"item\") != \"scroll\"))\n# Inverse\ndf_pl.filter(~pl.col(\"in_stock\"))\n# True if a value matches any of the specified values, else False\ndf_pl.filter(pl.col(\"item\").is_in([\"staff\", \"potion\"]))\n# Is not NA... consider also .is_not_null()\ndf_pl.filter(~pl.col(\"found_in\").is_null())"
  },
  {
    "objectID": "posts/my-pd-pl-guide/index.html#replacing-values-in-row-slices",
    "href": "posts/my-pd-pl-guide/index.html#replacing-values-in-row-slices",
    "title": "A Beginner’s Guide to pandas & polars",
    "section": "Replacing values in row slices",
    "text": "Replacing values in row slices\nLet me explain this with an example relating to the dataset at hand. We are looking at the inventory of a particular wizard shop. In this magical universe, let’s suppose that we learn that every item with magical_power over 900 is cursed. There might be other reasons why an item is cursed, but these reasons are unknown to us.\nWhat we can do is to filter the DataFrame to display only items with magical_power over 900 and using this filter, we write True to the is_cursed column for every row satisfying this condition.\n\n1️⃣ pandas2️⃣ polars\n\n\nLabel all items with magical_power over 900 as cursed.\n\n# A column full of NAs should be cast as \"object\" first\ndf_pd[\"is_cursed\"] = df_pd[\"is_cursed\"].astype(\"object\")\n\ndf_pd.loc[df_pd[\"magical_power\"] &gt; 900, \"is_cursed\"] = True\ndf_pd.head(3)\n\n\n\n\n\n\n\n\nid\nitem\nprice\nmagical_power\nquality\nin_stock\nfound_in\nis_cursed\nlog_price\n\n\n\n\n0\n1\namulet\n915.0\n402.961\n9\nTrue\ndungeon\nNaN\n6.818924\n\n\n1\n2\nstaff\n2550.0\n933.978\n2\nFalse\ndungeon\nTrue\n7.843849\n\n\n2\n3\npotion\n62.2\n129.897\n5\nTrue\ncity\nNaN\n4.130355\n\n\n\n\n\n\n\n\n\nLabel all items with magical_power over 900 as cursed.\n\ndf_pl = df_pl.with_columns(\n    pl.when(pl.col(\"magical_power\") &gt; 900)\n    .then(pl.lit(True))\n    .otherwise(pl.col(\"is_cursed\"))\n    .alias(\"is_cursed\")\n)\ndf_pl.head(3)\n\n\nshape: (3, 9)\n\n\n\nid\nitem\nprice\nmagical_power\nquality\nin_stock\nfound_in\nis_cursed\nlog_price\n\n\ni64\nstr\nf64\nf64\ni64\nbool\nstr\nbool\nf64\n\n\n\n\n1\n\"amulet\"\n915.0\n402.961\n9\ntrue\n\"dungeon\"\nnull\n6.818924\n\n\n2\n\"staff\"\n2550.0\n933.978\n2\nfalse\n\"dungeon\"\ntrue\n7.843849\n\n\n3\n\"potion\"\n62.2\n129.897\n5\ntrue\n\"city\"\nnull\n4.130355"
  },
  {
    "objectID": "posts/my-pd-pl-guide/index.html#create-a-copy",
    "href": "posts/my-pd-pl-guide/index.html#create-a-copy",
    "title": "A Beginner’s Guide to pandas & polars",
    "section": "Create a copy",
    "text": "Create a copy\n\n1️⃣ pandas2️⃣ polars\n\n\n\ndf_pd_temp = df_pd.copy()\n\n\n\nIn polars, copying or cloning (.clone() method) may not be necessary.5\n\ndf_pl_temp = df_pl"
  },
  {
    "objectID": "posts/my-pd-pl-guide/index.html#joining-data",
    "href": "posts/my-pd-pl-guide/index.html#joining-data",
    "title": "A Beginner’s Guide to pandas & polars",
    "section": "Joining data",
    "text": "Joining data\n\nInner join\nThe info_pd / info_pl table contains typical information (e.g., typical price) about each item in the fantasy universe this dataset was sourced from. Naturally, the shopkeeper has no incentive to provide this information in the original data. However, we can add the information from the info table to the main table ourselves.\nIn our case, we can use an “inner join” and match the items using the item column as long as\n\nboth tables contain this column,\nand the column itself serves the same purpose in both tables.\n\nIf there would be an additional item in the shopkeeper’s inventory for which we wouldn’t have data in the info table, we may consider using an “outer join.” For a review of joins consider reading the following Wiki article.\n\n1️⃣ pandas2️⃣ polars\n\n\n\ninfo_pd\n\n\n\n\n\n\n\n\nitem\ntypical_price\ntypical_quality\ntypically_found_in\n\n\n\n\n0\namulet\n1000\n9\ndungeon\n\n\n1\npotion\n50\n7\nvillage\n\n\n2\ncloak\n500\n4\ncity\n\n\n3\nstaff\n2000\n5\ndungeon\n\n\n4\nscroll\n900\n2\ndungeon\n\n\n\n\n\n\n\n\ndf_full_pd = pd.merge(left=df_pd, right=info_pd, how=\"inner\", on=\"item\")\ndf_full_pd.head(3)\n\n\n\n\n\n\n\n\nid\nitem\nprice\nmagical_power\nquality\nin_stock\nfound_in\nis_cursed\nlog_price\ntypical_price\ntypical_quality\ntypically_found_in\n\n\n\n\n0\n1\namulet\n915.0\n402.961\n9\nTrue\ndungeon\nNaN\n6.818924\n1000\n9\ndungeon\n\n\n1\n2\nstaff\n2550.0\n933.978\n2\nFalse\ndungeon\nTrue\n7.843849\n2000\n5\ndungeon\n\n\n2\n3\npotion\n62.2\n129.897\n5\nTrue\ncity\nNaN\n4.130355\n50\n7\nvillage\n\n\n\n\n\n\n\n\n\n\ninfo_pl\n\n\nshape: (5, 4)\n\n\n\nitem\ntypical_price\ntypical_quality\ntypically_found_in\n\n\nstr\ni64\ni64\nstr\n\n\n\n\n\"amulet\"\n1000\n9\n\"dungeon\"\n\n\n\"potion\"\n50\n7\n\"village\"\n\n\n\"cloak\"\n500\n4\n\"city\"\n\n\n\"staff\"\n2000\n5\n\"dungeon\"\n\n\n\"scroll\"\n900\n2\n\"dungeon\"\n\n\n\n\n\n\n\ndf_full_pl = df_pl.join(other=info_pl, on=\"item\", how=\"inner\")\ndf_full_pl.head(3)\n\n\nshape: (3, 12)\n\n\n\nid\nitem\nprice\nmagical_power\nquality\nin_stock\nfound_in\nis_cursed\nlog_price\ntypical_price\ntypical_quality\ntypically_found_in\n\n\ni64\nstr\nf64\nf64\ni64\nbool\nstr\nbool\nf64\ni64\ni64\nstr\n\n\n\n\n1\n\"amulet\"\n915.0\n402.961\n9\ntrue\n\"dungeon\"\nnull\n6.818924\n1000\n9\n\"dungeon\"\n\n\n2\n\"staff\"\n2550.0\n933.978\n2\nfalse\n\"dungeon\"\ntrue\n7.843849\n2000\n5\n\"dungeon\"\n\n\n3\n\"potion\"\n62.2\n129.897\n5\ntrue\n\"city\"\nnull\n4.130355\n50\n7\n\"village\"\n\n\n\n\n\n\n\n\n\n\n\nConcatenate by rows\nConcatenating by rows stacks two tables on top of each other like bricks.\n\n1️⃣ pandas2️⃣ polars\n\n\n\nnew_price = pd.DataFrame(\n    {\n        \"amulet\": [1005.1],\n        \"potion\": [55.32],\n        \"cloak\": [550.06],\n        \"staff\": [1500.15],\n        \"scroll\": [1123.06]\n    },\n    index=[datetime(2026, 1, 1)]\n)\npd.concat([prices_pd, new_price]).tail(2)\n\n\n\n\n\n\n\n\namulet\npotion\ncloak\nstaff\nscroll\n\n\n\n\n2025-12-31\n957.94\n64.08\n503.88\n2899.72\n829.59\n\n\n2026-01-01\n1005.10\n55.32\n550.06\n1500.15\n1123.06\n\n\n\n\n\n\n\n\n\n\nnew_price = pl.DataFrame({\n    \"date\": date(2026, 1, 1),\n    \"amulet\": 1005.1,\n    \"potion\": 55.32,\n    \"cloak\": 550.06,\n    \"staff\": 1500.15,\n    \"scroll\": 1123.06\n})\npl.concat([prices_pl, new_price]).tail(2)\n\n\nshape: (2, 6)\n\n\n\ndate\namulet\npotion\ncloak\nstaff\nscroll\n\n\ndate\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n2025-12-31\n957.94\n64.08\n503.88\n2899.72\n829.59\n\n\n2026-01-01\n1005.1\n55.32\n550.06\n1500.15\n1123.06"
  },
  {
    "objectID": "posts/my-pd-pl-guide/index.html#quick-plotting",
    "href": "posts/my-pd-pl-guide/index.html#quick-plotting",
    "title": "A Beginner’s Guide to pandas & polars",
    "section": "Quick plotting",
    "text": "Quick plotting\nDataFrames in pandas can be quickly plotted using the .plot() method. While polars also contains quick plotting capabilities, I prefer converting the DataFrame to pandas. For more complex plots, consider using seaborn, bokeh, plotly, altair, or any other data visualization library.\n\n1️⃣ pandas2️⃣ polars\n\n\nDisplay the mean quality of the shopkeeper’s items vs the typical quality of each item.\n\nquality_pd = df_full_pd[[\"item\", \"quality\", \"typical_quality\"]]\nquality_pd.groupby(\"item\").mean().plot(kind=\"bar\")\n\n\n\n\n\n\n\n\n\n\nDisplay the mean quality of the shopkeeper’s items vs the typical quality of each item.\n\nquality_pl = df_full_pl.to_pandas()[[\"item\", \"quality\", \"typical_quality\"]]\nquality_pl.groupby(\"item\").mean().plot(kind=\"bar\")\n\n\n\n\n\n\n\n\n# Similar plot in polars via Altair (not displayed)\n(\n    df_full_pl.group_by(\"item\")\n    .mean()\n    .select(\"item\", \"quality\", \"typical_quality\")\n    .unpivot(index=\"item\")\n    .plot.bar(x=\"item\", y=\"value\", color=\"variable\", column=\"variable\")\n)"
  },
  {
    "objectID": "posts/my-pd-pl-guide/index.html#dates",
    "href": "posts/my-pd-pl-guide/index.html#dates",
    "title": "A Beginner’s Guide to pandas & polars",
    "section": "Dates",
    "text": "Dates\n\nCreate a range of dates\nThe pandas package is great for time series data. It offers valuable functions like date_range() or to_datetime(). polars also has special capabilities for handling time series data, though it seems to me that it expects the user to utilize the datetime module more than pandas.\nAnother difference is that polars may return an expression instead of the actual Series of dates by default. This is why you’ll see me using eager=True to get a Series instead of an expression.\nLet’s take a look at some of the basic operations with dates.\n\n1️⃣ pandas2️⃣ polars\n\n\nMake a range of dates from a specific date (and time) to a specific date (and time).\n\npd.date_range(\n    start=\"2025-01-01 00:00\",\n    end=\"2025-01-01 23:00\", \n    freq=\"h\",\n    tz=\"Europe/Prague\"\n)\n\n\n\nMake a range of dates from a specific date (and time) to a specific date (and time).\n\npl.datetime_range(\n    start=datetime(2025, 1, 1, 0, 0), \n    end=datetime(2025, 1, 1, 23, 0),\n    interval=\"1h\",\n    eager=True,\n    time_zone=\"Europe/Prague\"\n)\n# Alternative start/end: datetime.fromisoformat(\"2025-01-01 00:00\")\n\n\n\n\n\n1️⃣ pandas2️⃣ polars\n\n\nCreate a date range with a specific number of periods. For instance, start at 2025-01-01 and continue 100 days into the future.\n\nout_periods = 100\n\npd.date_range(\n    start=\"2025-01-01\", \n    periods=out_periods, \n    freq=\"d\"\n)\n\n\n\nCreate a date range with a specific number of periods. For instance, start at 2025-01-01 and continue 100 days into the future.\n\nout_periods = 100\n\npl.date_range(\n    start=date(2025, 1, 1),\n    end=date(2025, 1, 1) + timedelta(days=out_periods - 1),\n    interval=\"1d\",\n    # Returns a Series if True, otherwise returns an expression\n    eager=True\n)\n\n\n\n\n\n\nSubset a specific time interval\nSuppose we want to take a look at February data in our price table.\n\n1️⃣ pandas2️⃣ polars\n\n\n\n# Prices in February\nprices_pd[prices_pd.index.month == 2].head()\n\n\n\n\n\n\n\n\namulet\npotion\ncloak\nstaff\nscroll\n\n\ndate\n\n\n\n\n\n\n\n\n\n2025-02-01\n1200.15\n45.70\n441.72\n1315.41\n604.27\n\n\n2025-02-02\n828.88\n55.32\n526.56\n1591.24\n628.25\n\n\n2025-02-03\n774.96\n47.35\n515.86\n1924.56\n969.40\n\n\n2025-02-04\n842.21\n53.05\n522.74\n2305.17\n991.64\n\n\n2025-02-05\n1147.03\n53.27\n583.07\n2222.09\n1529.96\n\n\n\n\n\n\n\n\n\n\n# Prices in February\nprices_pl.filter(pl.col(\"date\").dt.month() == 2).head()\n\n\nshape: (5, 6)\n\n\n\ndate\namulet\npotion\ncloak\nstaff\nscroll\n\n\ndate\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n2025-02-01\n1200.15\n45.7\n441.72\n1315.41\n604.27\n\n\n2025-02-02\n828.88\n55.32\n526.56\n1591.24\n628.25\n\n\n2025-02-03\n774.96\n47.35\n515.86\n1924.56\n969.4\n\n\n2025-02-04\n842.21\n53.05\n522.74\n2305.17\n991.64\n\n\n2025-02-05\n1147.03\n53.27\n583.07\n2222.09\n1529.96\n\n\n\n\n\n\n\n\n\n\n\nResampling\nIn time series analysis, we often need to change the frequency of our data. For instance, if we have daily data, we may want to look at monthly averages, yearly max values, and so on.\nConversely, suppose we want to join a daily and an hourly dataset. In this case, we may need to convert the daily time series to hourly. We can do this by, for example, repeating the daily value for each hour or by using interpolation.\n\n1️⃣ pandas2️⃣ polars\n\n\n\n# Monthly mean from daily data\nprices_pd.resample(\"ME\").mean()\n\n# Upsample daily data to hourly by repeating values\nprices_pd.resample(\"h\").ffill()\n\n# Upsample daily data to hourly by interpolating (linearly)\nprices_pd.resample(\"h\").interpolate(\"linear\")\n\n\n\n\n# Monthly mean from daily data\nprices_pl.group_by_dynamic(\n    index_column=\"date\",\n    every=\"1mo\"\n).agg(pl.selectors.numeric().mean())\n\n# Upsample daily data to hourly by repeating values\nprices_pl.upsample(\n    time_column=\"date\",\n    every=\"1h\"\n).select(pl.all().forward_fill())\n\n# Upsample daily data to hourly by interpolating (linearly)\nprices_pl.upsample(\n    time_column=\"date\",\n    every=\"1h\"\n).interpolate()\n\n\n\n\n\n\nGroup by time intervals\nWe can also group data by specific time intervals. For instance, we can calculate the median price of each item in 3-month windows.\n\n1️⃣ pandas2️⃣ polars\n\n\n\nprices_pd.groupby(pd.Grouper(freq=\"3MS\")).median()\n\n\n\n\n\n\n\n\namulet\npotion\ncloak\nstaff\nscroll\n\n\ndate\n\n\n\n\n\n\n\n\n\n2025-01-01\n973.755\n49.995\n503.050\n2043.170\n966.495\n\n\n2025-04-01\n990.040\n49.340\n505.000\n1931.890\n870.100\n\n\n2025-07-01\n1051.605\n50.345\n499.150\n2005.865\n850.470\n\n\n2025-10-01\n991.855\n49.620\n509.435\n1997.755\n851.620\n\n\n\n\n\n\n\n\n\nSome people (example in the official docs) format longer polars code like this:\n\n(\n    prices_pl\n    .group_by_dynamic(index_column=\"date\", every=\"3mo\")\n    .agg(pl.selectors.numeric().median())\n)\n\n\nshape: (4, 6)\n\n\n\ndate\namulet\npotion\ncloak\nstaff\nscroll\n\n\ndate\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n2025-01-01\n973.755\n49.995\n503.05\n2043.17\n966.495\n\n\n2025-04-01\n990.04\n49.34\n505.0\n1931.89\n870.1\n\n\n2025-07-01\n1051.605\n50.345\n499.15\n2005.865\n850.47\n\n\n2025-10-01\n991.855\n49.62\n509.435\n1997.755\n851.62"
  },
  {
    "objectID": "posts/my-pd-pl-guide/index.html#saving-data",
    "href": "posts/my-pd-pl-guide/index.html#saving-data",
    "title": "A Beginner’s Guide to pandas & polars",
    "section": "Saving data",
    "text": "Saving data\nBoth packages support a large number of output formats and the syntax is similar.\n\n1️⃣ pandas2️⃣ polars\n\n\ndf_pd.to_csv(\"file.csv\")\n\n\ndf_pl.write_csv(\"file.csv\")"
  },
  {
    "objectID": "posts/my-pd-pl-guide/index.html#sources-and-further-reading",
    "href": "posts/my-pd-pl-guide/index.html#sources-and-further-reading",
    "title": "A Beginner’s Guide to pandas & polars",
    "section": "Sources and further reading",
    "text": "Sources and further reading\nThis guide is mainly based on information from\n\nhttps://docs.pola.rs/\nhttps://pandas.pydata.org/docs/\n\nIt’s also loosely inspired by Keith Galli’s incredible pandas tutorial on YouTube\n\nhttps://www.youtube.com/watch?v=vmEHCJofslg\n\nLet me also mention a neat cheat sheet on going from pandas to polars on Rho Signal\n\nhttps://www.rhosignal.com/posts/polars-pandas-cheatsheet/\n\nFinally, for more advanced and extensive coverage of polars x pandas, I would highly recommend “Modern Polars”\n\nhttps://kevinheavey.github.io/modern-polars/\n\n Comment on GitHub"
  },
  {
    "objectID": "posts/my-pd-pl-guide/index.html#footnotes",
    "href": "posts/my-pd-pl-guide/index.html#footnotes",
    "title": "A Beginner’s Guide to pandas & polars",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor stylistic purposes, let me refer to the Polars package as polars.↩︎\n33.4k stars in early May 2025 https://github.com/pola-rs/polars.↩︎\nJetBrains (2023). “What is Polars?” on YouTube. https://www.youtube.com/watch?v=QfLzEp-yt_U.↩︎\nSee https://github.com/rnd195/wizard-shop-dataset↩︎\nSee https://stackoverflow.com/a/78136839↩︎"
  },
  {
    "objectID": "posts/tfm-electricity-consumption/index.html",
    "href": "posts/tfm-electricity-consumption/index.html",
    "title": "Benchmarking TimesFM on Electricity Consumption Data",
    "section": "",
    "text": "Generating forecasts of Finnish electricity consumption using the TimesFM 200M model by Google Research\nIn early 2024, Google Research announced TimesFM, a pre-trained univariate time-series foundation model for time-series forecasting.1 First, let me break down the terminology. The general idea is that this is a model for sequences of data that are ordered in time (time-series). A few examples of time-series include: annual inflation rate, monthly precipitation, weekly water demand, or hourly temperatures. This being a forecasting model simply means that we are interested in predicting the future. Univariate implies that this model only takes in and outputs a single time-series—in other words, no other variables are utilized. Pre-trained means, for our purposes, that we don’t need to run the complex process of training the model. We simply download a large file that contains all the 200M parameters for the model to produce forecasts. Finally, a foundation model is one that has been pre-trained on a large number of diverse datasets.2\nTo be honest, when I first read about this model, I realized that I have never used a pre-trained time-series model. And it’s not like these didn’t exist before. In fact, several pre-trained foundation models have been released (see, for example3). But seeing Google Research announce their pre-trained time-series forecasting model put this whole category of models on my radar.\nIn their paper on TimesFM,4 the authors tested the model’s performance on popular time-series datasets and demonstrated that TimesFM was, in the majority of cases, able to outperform both classical and deep learning models using their methodology. For this reason, I was interested in knowing how well it would forecast something I am familiar with—electricity consumption."
  },
  {
    "objectID": "posts/tfm-electricity-consumption/index.html#data",
    "href": "posts/tfm-electricity-consumption/index.html#data",
    "title": "Benchmarking TimesFM on Electricity Consumption Data",
    "section": "Data",
    "text": "Data\nSpecifically, I decided to use electricity consumption data from the operator of the Finnish grid through their Open Data portal.5 At this point, I believe that it is important to give my words of appreciation to the people at Fingrid for providing easy access to the data with clear license terms. In my view, this is the way open data should be done, so thank you!\nFrom my experience, electricity consumption in (most of?) Europe typically follows these three major seasonal patterns:\n\n\n\n\n\n\nFigure 1: Simplified electricity consumption pattern in (most of?) Europe\n\n\n\nFirstly, there’s typically high electricity consumption in winter, lowering in spring, low during summer months, and slowly climbing upward in autumn. Note that the dip at the end of the year is due to winter holidays and it can be quite substantial. Secondly, on a typical work week, the Monday to Friday pattern looks fairly similar, but there is lower electricity consumption during the weekend. Finally, if we look at a single day’s worth of electricity consumption data, there will likely be less electricity consumed at night and consumption will peak around midday, perhaps in the afternoon, or early evening.\nIf we look at hourly electricity consumption in Finland (resampled from 15-min by averaging), I can cherry-pick specific weeks or days to make the series look similar to the patterns illustrated above. However, the full 2024 snapshot makes it clear that electricity consumption patterns in Finland are more nuanced.\n\n\n\n\n\n\nFigure 2: Hourly electricity consumption in Finland (MWh), 2024"
  },
  {
    "objectID": "posts/tfm-electricity-consumption/index.html#setup",
    "href": "posts/tfm-electricity-consumption/index.html#setup",
    "title": "Benchmarking TimesFM on Electricity Consumption Data",
    "section": "Setup",
    "text": "Setup\nI will be benchmarking the 200M parameter model on the whole 2024 hourly consumption data illustrated above. Naturally, we already know what the 2024 electricity consumption looks like, but it would be highly impractical to produce forecasts from the current point in time onward while waiting for the future to materialize to collect the data and determine the accuracy of our predictions. Instead, we can go step by step, limiting the knowledge our forecasting model takes in, generate forecasts at that point in time, and immediately compare against the actual values. This is called pseudo-out-of-sample forecasting (see, for example6).\nIn electricity consumption forecasting, it often makes sense to think about the data in full days, not just individual hours. Therefore, I will be forecasting up to 24 hours ahead following the scheme described below:\n\nStart on Jan 1st 2024, 00:00 – this will be the first prediction\nWith no knowledge of electricity consumption on Jan 1st, produce forecasts up to 23:00 (included)\nSave these forecasts and move by one day\nGenerate predictions for Jan 2nd (00:00, 01:00, …, 23:00)\nMove by one day\nContinue until you reach Dec 31st 2024\n\nBelow is an illustration of the scheme—the three vertical lines are the starting points for each 24-hour prediction as well as a knowledge-cutoff line. The dashed curves illustrate predictions while the solid line is meant to represent actual values that are not “seen” by the model beforehand.\n\n\n\n\n\n\nFigure 3: Forecasting scheme illustration\n\n\n\nTo produce the forecasts, I am using the official timesfm Python package and the package-specific code only really takes a few lines if we leave all other hyperparameters set to their default values.\n# Up to 24 hours ahead\nhorizon = 24\n# Define model parameters\nmodel_params = timesfm.TimesFmHparams(\n    backend=\"gpu\",\n    horizon_len=horizon\n)\n# Load checkpoint from huggingface\nhf_checkpoint = timesfm.TimesFmCheckpoint(\n    huggingface_repo_id=\"google/timesfm-1.0-200m-pytorch\"\n)\n# Setup the model\nmodel = timesfm.TimesFm(\n    hparams=model_params,\n    checkpoint=hf_checkpoint\n)\nAssuming that our data is stored in a pandas DataFrame, we can produce forecasts by running\nfcst = model.forecast_on_df(\n    # Formatted dataframe\n    inputs=df,\n    # Data frequency (hourly)\n    freq=\"h\",\n    # Target variable column name\n    value_name=\"consumption\",\n    # -1 sets the number of processes to cpu count\n    num_jobs=-1\n)\nAnd that’s about it. You may find the whole implementation in a Jupyter notebook linked in the floating table of contents on the right (or here)."
  },
  {
    "objectID": "posts/tfm-electricity-consumption/index.html#results",
    "href": "posts/tfm-electricity-consumption/index.html#results",
    "title": "Benchmarking TimesFM on Electricity Consumption Data",
    "section": "Results",
    "text": "Results\nFirstly, let me briefly summarize the setup and all the parameters used to generate the forecasts:\n\nData\n\nelectricity consumption in Finland in MWh\nhourly frequency (resampled from 15-min by averaging)\nstart 2024-01-01 00:00, end 2024-12-31 23:00 (excluding some missing values)\n\nForecasting scheme\n\nup to 24 hours ahead\nstart at 00:00, generate predictions up to 23:00 each day, and move to the next day\n\nModel\n\nTimesFM 200M with default parameters\n\n\n\nPlots\nFor visual convenience, I connected all the forecasts into a single series. However, let me once again note that the predictions were generated for each day separately (denoted by the vertical gridlines).\nWith that in mind, let’s take a look at some of the forecasts (in red) versus actual values (in blue), starting in the first week of Jan 2024:\n\n\n\n\n\n\nFigure 4: Forecasts vs actual values (first week of Jan 2024)\n\n\n\nMy first thoughts are that the model is somewhat capable of capturing the general day/night pattern as well as the rough daily consumption level (with some exceptions). However, it seems to struggle with the shape of the peak hours (midday/afternoon) for most days of the week. Of course, this is just a single week out of the whole year, but these symptoms appear to be prevalent in most weeks of the year as suggested by the monthly error metrics displayed later (Figure 7 and Figure 8).\nBelow is a snapshot of early July 2024 forecasts:\n\n\n\n\n\n\nFigure 5: Forecasts vs actual values (first week of Jul 2024)\n\n\n\nThis is a totally different story as it seems that the model was capable of generating relatively accurate forecasts. This can likely be attributed to the less volatile pattern of electricity consumption in the summer months of 2024 which is certainly helpful for univariate models.\nFinally, let’s take a look at a sample from October 2024:\n\n\n\n\n\n\nFigure 6: Forecasts vs actual values (first week of Oct 2024)\n\n\n\nIn this case, we get similar issues with the shape and sometimes even the level as in the first displayed plot. Interestingly, perhaps due to the autoregressive nature of the model, it was able to capture the distinct “U” shape of the peak consumption.\n\n\nError metrics\nUsing the forecasting scheme outlined above, the 200M TimesFM model with default parameters was capable of recording the following accuracy metrics on the 2024 Finnish electricity consumption data\n\n\n\n\n\n\n\n\nMetric (total)\nValue\nExplainer\n\n\n\n\nMean absolute percentage error (MAPE)\n2.659%\nOn average, how much does a single point forecast deviate from actual values in absolute terms. Denoted in %\n\n\nRoot mean square error (RMSE)\n356.89071 MWh\nOn average, how much does a single point forecast deviate from actual values in squared terms. Larger mistakes are penalized more. Denoted in MWh\n\n\nR-squared\n95.817%\nThis can help determine shape accuracy, but not level\n\n\nMean bias error\n18.25352 MWh\nOverestimation (+) or underestimation (-), on average\n\n\n\nLet’s also take a look at MAPE and RMSE grouped by month to get a better picture of which months were easier to forecast with the model.\n\n\n\n\n\n\n\n\n\nFigure 7: Monthly MAPE\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Monthly RMSE\n\n\n\n\n\n\nIt is clear that the 200M TimesFM model was the most accurate in July and August which can partly be seen in Figure 5 as discussed earlier. In terms of RMSE, the least accurate forecasts were generated in the first month of 2024."
  },
  {
    "objectID": "posts/tfm-electricity-consumption/index.html#thoughts",
    "href": "posts/tfm-electricity-consumption/index.html#thoughts",
    "title": "Benchmarking TimesFM on Electricity Consumption Data",
    "section": "Thoughts",
    "text": "Thoughts\nWhile I cannot confidently comment on the relative levels of these error metrics with respect to Finnish data due to a lack of experience with Finnish electricity consumption, I find these results impressive for a univariate model that doesn’t need to be trained. It is without a doubt that multivariate models would outperform this framework in this particular forecasting exercise. Still, I think it is valuable to benchmark the model on data that is known to be forecastable as it may help us understand what we can expect from the framework in other domains.\nRegardless, I am now very interested in future foundation time-series models that Google Research may release. Perhaps this is a sign of what’s to come in the time-series forecasting domain? I hope so. On that note, the TimesFM repository on GitHub has recently been updated with a 500M variant. Would this model generate more accurate forecasts in the exercise outlined in this post? It’s possible but that is a question for another time.\n Comment on GitHub Notebook with code"
  },
  {
    "objectID": "posts/tfm-electricity-consumption/index.html#footnotes",
    "href": "posts/tfm-electricity-consumption/index.html#footnotes",
    "title": "Benchmarking TimesFM on Electricity Consumption Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGoogle Research (2024). A decoder-only foundation model for time-series forecasting. https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/↩︎\nSee, for example https://www.ibm.com/think/topics/foundation-models↩︎\nFor instance https://github.com/qianlima-lab/time-series-ptms?tab=readme-ov-file#pre-trained-models-on-time-series-forecasting↩︎\nDas, A., Kong, W., Sen, R., & Zhou, Y. (2023). A decoder-only foundation model for time-series forecasting. arXiv preprint arXiv:2310.10688. https://arxiv.org/abs/2310.10688↩︎\n“Electricity consumption in Finland” provided by Fingrid is licensed under CC BY 4.0↩︎\nHanck, C., Arnold, M., Gerber, A., & Schmelzer, M. (2024). Introduction to Econometrics with R. https://www.econometrics-with-r.org/14.8-niib.html#pseudo-out-of-sample-forecasting↩︎"
  }
]